{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "350c7398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9da9c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qU \"langchain[groq]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ad46e3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_groq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGroq\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m ChatGroq(model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3-8b-8192\u001b[39m\u001b[38;5;124m\"\u001b[39m, api_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgsk_LlHk9wm1PtvxI9aYwwxlWGdyb3FYytHPABeRI5na5lA50PmqyTBg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m model\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA bat and a ball together cost $1.10. The bat costs $1.00 more than the ball. How much does the ball cost?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThis question requires logical thinking, problem-solving, and the ability to avoid intuitive but incorrect answers. Can you figure it out?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\atulk\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:383\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    379\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    380\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    382\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 383\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    384\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    385\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    386\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    387\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    388\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    389\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    390\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    391\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    392\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    393\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\atulk\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1006\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    997\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1003\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1004\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m   1005\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m-> 1006\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\atulk\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:825\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    824\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 825\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    826\u001b[0m                 m,\n\u001b[0;32m    827\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    828\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    829\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    830\u001b[0m             )\n\u001b[0;32m    831\u001b[0m         )\n\u001b[0;32m    832\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    833\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\atulk\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1072\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1070\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1072\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m   1073\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1074\u001b[0m     )\n\u001b[0;32m   1075\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1076\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\atulk\\anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:527\u001b[0m, in \u001b[0;36mChatGroq._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    522\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    523\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    524\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    526\u001b[0m }\n\u001b[1;32m--> 527\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response, params)\n",
      "File \u001b[1;32mc:\\Users\\atulk\\anaconda3\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:378\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    233\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    234\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    235\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    379\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/openai/v1/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    380\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    381\u001b[0m             {\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexclude_domains\u001b[39m\u001b[38;5;124m\"\u001b[39m: exclude_domains,\n\u001b[0;32m    385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    386\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    387\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_domains\u001b[39m\u001b[38;5;124m\"\u001b[39m: include_domains,\n\u001b[0;32m    389\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_reasoning\u001b[39m\u001b[38;5;124m\"\u001b[39m: include_reasoning,\n\u001b[0;32m    390\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    391\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    392\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m    393\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    394\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    395\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    396\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    397\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    398\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[0;32m    399\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_format,\n\u001b[0;32m    400\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    401\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch_settings\u001b[39m\u001b[38;5;124m\"\u001b[39m: search_settings,\n\u001b[0;32m    402\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    403\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    404\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    405\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m    406\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    407\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    408\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    409\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    410\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    411\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    412\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    413\u001b[0m             },\n\u001b[0;32m    414\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    415\u001b[0m         ),\n\u001b[0;32m    416\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    417\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    418\u001b[0m         ),\n\u001b[0;32m    419\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    420\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    421\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    422\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\atulk\\anaconda3\\Lib\\site-packages\\groq\\_base_client.py:1242\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1230\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1239\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1240\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1241\u001b[0m     )\n\u001b[1;32m-> 1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32mc:\\Users\\atulk\\anaconda3\\Lib\\site-packages\\groq\\_base_client.py:1044\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1043\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1044\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model = \"llama3-8b-8192\", api_key = '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')\n",
    "\n",
    "model.invoke(\"A bat and a ball together cost $1.10. The bat costs $1.00 more than the ball. How much does the ball cost?\\n\\nThis question requires logical thinking, problem-solving, and the ability to avoid intuitive but incorrect answers. Can you figure it out?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2b2c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Playing basketball is a fun and exciting sport that requires some basic skills and knowledge of the rules. Here's a step-by-step guide on how to play basketball:\\n\\n**Objective:**\\nThe objective of basketball is to score more points than the opposing team by shooting the ball into the opponent's basket.\\n\\n**Equipment:**\\n\\n* Basketball court with a rectangular shape and free throw lines\\n* Basketball hoop with a net\\n* Basketball\\n* Players (5 on each team)\\n\\n**Basic Rules:**\\n\\n1. The game is divided into four quarters, each lasting 12 minutes in professional play and 8 minutes in college and high school play.\\n2. The ball must be advanced by dribbling or passing, not by carrying or throwing it.\\n3. Players can only hold the ball for 5 seconds before passing, shooting, or dribbling again.\\n4. Teams must shoot the ball within 24 seconds of gaining possession.\\n5. Players can only travel once before passing or shooting the ball.\\n6. Fouls, such as holding, pushing, or tripping, can result in free throws or possession of the ball.\\n\\n**Basic Skills:**\\n\\n1. **Dribbling:** Hold the ball with your fingertips, keeping your elbows close to your body. Move your hands in a circular motion to control the ball.\\n2. **Passing:** Use your fingertips to pass the ball to a teammate. Aim for the player's hands or shoulders.\\n3. **Shooting:** Hold the ball with your dominant hand, keeping your elbow at a 90-degree angle. Aim for the center of the rim.\\n4. **Defense:** Use your feet to move quickly and stay in front of your opponent. Use your hands to block shots and steal the ball.\\n\\n**Gameplay:**\\n\\n1. The game starts with a jump ball, where two players from each team jump up and try to tap the ball to a teammate.\\n2. Players take turns possession of the ball, trying to score or advance it down the court.\\n3. Teams can score by shooting the ball into the opponent's basket (2 points) or by making a free throw (1 point).\\n4. Players can also score by making a slam dunk (2 points) or a three-point shot (3 points).\\n5. The team with the ball is called the offense, while the team without the ball is called the defense.\\n6. Players can use various tactics, such as screens, backdoor cuts, and pick-and-rolls, to create scoring opportunities.\\n\\n**Common Positions:**\\n\\n1. **Point Guard:** The team's primary ball handler and playmaker.\\n2. **Shooting Guard:** A scoring specialist who can shoot from outside the three-point line.\\n3. **Small Forward:** A versatile player who can score, rebound, and play defense.\\n4. **Power Forward:** A rebounding and post-up specialist.\\n5. **Center:** The team's tallest player, responsible for defending the basket and rebounding.\\n\\n**Tips for Beginners:**\\n\\n1. Practice your dribbling, passing, and shooting skills.\\n2. Focus on basic defense, such as staying in front of your opponent and blocking shots.\\n3. Learn to move quickly and efficiently up and down the court.\\n4. Communicate with your teammates to set up screens and create scoring opportunities.\\n5. Stay positive and have fun!\\n\\nRemember, the key to becoming a good basketball player is to practice regularly and consistently. Start with basic drills and gradually work your way up to more advanced skills. Most importantly, have fun and enjoy the game!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 719, 'prompt_tokens': 14, 'total_tokens': 733, 'completion_time': 0.527635473, 'prompt_time': 0.002091024, 'queue_time': 0.044878016, 'total_time': 0.529726497}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_4b5fbf0ced', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--6c1b0d61-a40c-44a2-9ce1-8612985af76d-0', usage_metadata={'input_tokens': 14, 'output_tokens': 719, 'total_tokens': 733})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"How to play basketball\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939354f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi! It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 11, 'total_tokens': 36, 'completion_time': 0.018009093, 'prompt_time': 0.001696861, 'queue_time': 0.046841829, 'total_time': 0.019705954}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_4b5fbf0ced', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--c60544ca-ca30-4219-b1a1-5dd7b0c0fe0d-0', usage_metadata={'input_tokens': 11, 'output_tokens': 25, 'total_tokens': 36})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa22d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object BaseChatModel.stream at 0x0000026FE005E8C0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.stream(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7158ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 11, 'total_tokens': 37, 'completion_time': 0.018970258, 'prompt_time': 0.00185196, 'queue_time': 0.04771718, 'total_time': 0.020822218}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_4b5fbf0ced', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--5ce0e8f4-3136-4ab2-ac75-6be35ca80a26-0', usage_metadata={'input_tokens': 11, 'output_tokens': 26, 'total_tokens': 37})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.invoke(\"hello\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c4fedea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here\\'s a question to challenge someone\\'s cognitive abilities:\\n\\n\"A bat and a ball together cost $1.10. The bat costs $1.00 more than the ball. How much does the ball cost?\"\\n\\nThis question requires logical reasoning, mathematical thinking, and the ability to avoid intuitive but incorrect answers. Can you figure it out?\\n\\n(Note: I\\'ll be happy to provide the answer and explanation if you need it!)', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 85, 'prompt_tokens': 44, 'total_tokens': 129, 'completion_time': 0.241005468, 'prompt_time': 0.011229537, 'queue_time': 0.049262433, 'total_time': 0.252235005}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_2ddfbb0da0', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--feca362f-c92d-4322-8be9-3294268bd0ce-0', usage_metadata={'input_tokens': 44, 'output_tokens': 85, 'total_tokens': 129})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model = \"llama-3.3-70b-versatile\", api_key = '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')\n",
    "\n",
    "model.invoke(\" generate a question to check someone's IQ.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddc26f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Playing basketball can be a fun and rewarding experience, whether you're a beginner or an experienced player. Here's a step-by-step guide to help you get started:\\n\\n**Objective:**\\nThe objective of basketball is to score more points than the opposing team by shooting the ball into the opponent's basket.\\n\\n**Basic Rules:**\\n\\n1. The game is played by two teams, each with five players on the court.\\n2. The game is divided into four quarters, each lasting 12 minutes in professional play and 8 minutes in college and high school play.\\n3. The team with the ball is called the offense, and they try to score by bringing the ball up the court and shooting it into the opponent's basket.\\n4. The team without the ball is called the defense, and they try to stop the other team from scoring by stealing the ball or blocking shots.\\n\\n**Basic Skills:**\\n\\n1. **Dribbling:** Dribbling is the act of bouncing the ball on the floor while moving. To dribble, start by standing with your feet shoulder-width apart and your knees slightly bent. Hold the ball with your fingertips, and bounce it gently on the floor in front of you.\\n2. **Shooting:** Shooting is the act of throwing the ball into the opponent's basket. To shoot, stand with your feet shoulder-width apart and your knees slightly bent. Hold the ball with your fingertips, and push it upwards towards the basket.\\n3. **Passing:** Passing is the act of throwing the ball to a teammate. To pass, stand with your feet shoulder-width apart and your knees slightly bent. Hold the ball with your fingertips, and throw it to your teammate.\\n4. **Rebounding:** Rebounding is the act of grabbing the ball after a shot has been taken. To rebound, stand near the basket and jump up to grab the ball.\\n\\n**Gameplay:**\\n\\n1. **Tip-off:** The game starts with a tip-off, where two players from each team jump up and try to tap the ball to a teammate.\\n2. **Bringing the ball up the court:** The team with the ball brings it up the court by dribbling or passing it to teammates.\\n3. **Scoring:** The team with the ball tries to score by shooting the ball into the opponent's basket.\\n4. **Defending:** The team without the ball tries to stop the other team from scoring by stealing the ball or blocking shots.\\n5. **Rebounding:** After a shot has been taken, players from both teams try to grab the ball and gain possession.\\n\\n**Positions:**\\n\\n1. **Point Guard (PG):** The point guard is usually the team's best ball handler and is responsible for bringing the ball up the court.\\n2. **Shooting Guard (SG):** The shooting guard is usually the team's best shooter and is responsible for scoring from the perimeter.\\n3. **Small Forward (SF):** The small forward is usually the team's most versatile player and is responsible for scoring, rebounding, and defending.\\n4. **Power Forward (PF):** The power forward is usually the team's strongest player and is responsible for scoring and rebounding in the paint.\\n5. **Center (C):** The center is usually the team's tallest player and is responsible for defending the basket and rebounding.\\n\\n**Tips for Beginners:**\\n\\n1. **Practice your dribbling:** Start by practicing your dribbling in a stationary position, and then try moving around while dribbling.\\n2. **Practice your shooting:** Start by practicing your shooting from a stationary position, and then try moving around while shooting.\\n3. **Play with others:** Playing with others is a great way to improve your skills and learn the game.\\n4. **Watch and learn:** Watch professional and college basketball games to learn new skills and strategies.\\n\\nRemember, the key to improving your basketball skills is to practice regularly and have fun!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 796, 'prompt_tokens': 40, 'total_tokens': 836, 'completion_time': 1.472288565, 'prompt_time': 0.010874389, 'queue_time': 0.046054301, 'total_time': 1.483162954}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_6507bcfb6f', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--5607d26c-1c2d-44bd-aedc-1ffe4b3a94b4-0', usage_metadata={'input_tokens': 40, 'output_tokens': 796, 'total_tokens': 836})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"how to play basketball?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc793af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "\n",
    "prompt_template = \"\"\"Translate the given text in to the given language: \n",
    "Text: {text}\n",
    "Lang: {lang}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a916c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the given text in to the given language: \n",
      "Text: why are you attending llm lecture?\n",
      "Lang: spanish\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(text = \"why are you attending llm lecture?\", lang = \"spanish\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93ab9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Here's the translation:\\n\\nTexto: ¿Por qué estás asistiendo a la clase de LL.M.?\\n\\nNote: LL.M. stands for Master of Laws, so I've translated the text to ask about attending a Master of Laws lecture in Spanish.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 35, 'total_tokens': 88, 'completion_time': 0.040201733, 'prompt_time': 0.013222986, 'queue_time': 0.529549442, 'total_time': 0.053424719}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_4b5fbf0ced', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--f6db027c-b7ad-4539-8c5d-cb2353dec58c-0', usage_metadata={'input_tokens': 35, 'output_tokens': 53, 'total_tokens': 88})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response =  model.invoke(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d229a5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the translation:\n",
      "\n",
      "Texto: ¿Por qué estás asistiendo a la clase de LL.M.?\n",
      "\n",
      "Note: LL.M. stands for Master of Laws, so I've translated the text to ask about attending a Master of Laws lecture in Spanish.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0593983",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63acaf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the context, Lionel Messi is considered one of the greatest football players of all time, known for his skills and playmaking abilities, and ranking among the top scorers in history.\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(question = \"Who is the greatest football player of all time?\", context = \"Messi has got skills and a very good playmaker in the history of football and comest in the top scorer of all time\")\n",
    "\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f0ad87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The situation between the US and India has become increasingly tense due to a trade dispute and India's decision to continue buying oil from Russia. Here are the key points:\n",
      "\n",
      "1. **US tariffs on India**: The US has imposed tariffs of 50% on goods from India, which includes a 25% penalty for India's refusal to stop buying oil from Russia. These tariffs took effect last week.\n",
      "2. **India's response**: India has not officially responded to the latest comments from US President Donald Trump, but the country's commerce minister, Piyush Goyal, has stated that India will not bow down or appear weak in its economic relationships with other countries.\n",
      "3. **Trump's comments**: Trump has called the current trade relationship between the US and India a \"totally one-sided disaster\" and claimed that India has offered to cut its tariffs to zero. He also stated that India buys most of its oil and military products from Russia, and very little from the US.\n",
      "4. **India's oil imports**: India has defended its decision to continue buying oil from Russia, citing the need to meet the energy needs of its vast population. The country has called the US tariffs \"unfair, unjustified, and unreasonable\".\n",
      "5. **Geopolitical implications**: The dispute has caused a strain in Delhi-Washington ties, which are already at an all-time low due to India's decision to continue buying oil from Russia despite US pressure to stop. Indian Prime Minister Narendra Modi's attendance at the Shanghai Cooperation Organisation (SCO) summit, where he met with Chinese President Xi Jinping and Russian President Vladimir Putin, has further highlighted the tensions between the US and India.\n",
      "6. **Trade imbalance**: Trump has highlighted the trade imbalance between the US and India, stating that India sells massive amounts of goods to the US, but the US sells very little to India. He has called for a more balanced trade relationship between the two countries.\n",
      "\n",
      "Overall, the situation between the US and India remains tense, with both countries dug in on their positions. The dispute has significant implications for the global economy and geopolitics, particularly in the context of the ongoing conflict in Ukraine.\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"US President Donald Trump says India has offered to cut its tariffs to nothing even as he called the current trade stalemate with the country a totally one sided disaster.US tariffs of 50% on goods from India - which includes 25% penalty for Delhi's refusal to stop buying oil from Russia - took effect last week.India has not responded to Trump's latest comment but such war of words over Russian oil has caused Delhi-Washington ties to hit an all-time low.Trump's comment coincides with Indian Prime Minister Narendra Modi attending the Shanghai Co-operation Organisation (SCO) summit in Tianjin where he met Chinese President Xi Jinping and Russian President Vladimir Putin.Washington says Delhi has been indirectly funding Russia's war in Ukraine.India buys most of its oil and military products from Russia, very little from the US, Trump wrote, adding Delhi should have cut tariffs years ago.Delhi has previously said that oil supply from Russia was vital to meet the energy needs of its vast populationIt has also called the tariffs unfair, unjustified and unreasonable.Last week, the country's commerce minister, Piyush Goyal, said India will neither bow down nor ever appear weak in its economic relationships with other countries.He also said the country was ready to a have a free-trade agreement with anyone who wanted it.On Monday, Trump wrote: What few people understand is that we do very little business with India, but they do a tremendous amount of business with us. In other words, they sell us massive amounts of goods, their biggest client, but we sell them very little - Until now a totally one sided relationship, and it has been for many decades.The Indian PM said he had an insightful exchange with Putin.\"\n",
    "\n",
    "prompt = prompt_template.format(question = \"Give  me the summary of the news article?\", context = \"new context\")\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model = \"llama-3.3-70b-versatile\", api_key = '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')\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb59de6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model = \"llama-3.3-70b-versatile\", api_key = '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')\n",
    "\n",
    "with open(\"book.txt\", 'r', encoding = 'utf-8') as file:\n",
    "    book_context = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d1b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file = open(\"book.txt\", encoding='utf-8')\n",
    "book_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "418c2560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cd328ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model = \"llama-3.3-70b-versatile\", api_key = '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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b87dcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"book.txt\", encoding='utf-8')\n",
    "text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c8b53dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Voici une liste des titres d'histoire avec un court résumé en français pour chaque histoire :\\n\\n1. **No. 34. SATURDAY, MARCH 3, 1753.** \\nRésumé : L'auteur présente son histoire, celle de Misargyrus, qui a vécu une vie de libertinage et d'extravagance, mais qui est maintenant en prison et qui veut partager son histoire pour aider les autres à éviter les mêmes erreurs.\\n\\n2. **No. 39. TUESDAY, MARCH 20, 1753.** \\nRésumé : L'auteur réflexe sur le sommeil et son importance pour la vie humaine, notant que le sommeil est souvent négligé et sous-estimé, mais qu'il est essentiel pour la santé et le bien-être.\\n\\n3. **No. 41. TUESDAY, MARCH 27, 1753.** \\nRésumé : Misargyrus continue son histoire, racontant comment il a tenté d'obtenir de l'argent pour payer ses dettes, mais a été trompé par des usuriers et des prêteurs sans scrupules, ce qui l'a conduit à une situation financière désespérée.\\n\\nLes autres histoires ne sont pas fournies dans le texte, donc je ne peux pas les résumer.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 305, 'prompt_tokens': 10683, 'total_tokens': 10988, 'completion_time': 0.958415956, 'prompt_time': 0.793843329, 'queue_time': 0.053531549, 'total_time': 1.752259285}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_2ddfbb0da0', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--65088122-492e-4350-8b5e-0a316b04b5dd-0' usage_metadata={'input_tokens': 10683, 'output_tokens': 305, 'total_tokens': 10988}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\"llama-3.3-70b-versatile\", model_provider=\"groq\")\n",
    "\n",
    "prompt_template = \"\"\"You are a novel reader. You are given collection of stories:\n",
    "{collection_of_stories}\n",
    "You are tasked to make a list of story titles in this collection. Write a short summary for each story in french Language. Skip the story from the list, if the story is not provided in the text. \n",
    "\"\"\"\n",
    "\n",
    "response = model.invoke(prompt_template.format(collection_of_stories = text[:len(text)//20]))\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bdb4f061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Voici une liste des titres d'histoire avec un court résumé en français pour chaque histoire :\n",
       "\n",
       "1. **No. 34. SATURDAY, MARCH 3, 1753.** \n",
       "Résumé : L'auteur présente son histoire, celle de Misargyrus, qui a vécu une vie de libertinage et d'extravagance, mais qui est maintenant en prison et qui veut partager son histoire pour aider les autres à éviter les mêmes erreurs.\n",
       "\n",
       "2. **No. 39. TUESDAY, MARCH 20, 1753.** \n",
       "Résumé : L'auteur réflexe sur le sommeil et son importance pour la vie humaine, notant que le sommeil est souvent négligé et sous-estimé, mais qu'il est essentiel pour la santé et le bien-être.\n",
       "\n",
       "3. **No. 41. TUESDAY, MARCH 27, 1753.** \n",
       "Résumé : Misargyrus continue son histoire, racontant comment il a tenté d'obtenir de l'argent pour payer ses dettes, mais a été trompé par des usuriers et des prêteurs sans scrupules, ce qui l'a conduit à une situation financière désespérée.\n",
       "\n",
       "Les autres histoires ne sont pas fournies dans le texte, donc je ne peux pas les résumer."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41e8e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"book.txt\")\n",
    "docs = loader.load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
